{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "with open(\n",
    "    \"C:/Users/syafi/Desktop/syafiq-project/new classification task/model/saved_data/preprocessed_data.pkl\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    X_train, X_test, y_train, y_test = pickle.load(f)\n",
    "\n",
    "with open(\n",
    "    \"C:/Users/syafi/Desktop/syafiq-project/new classification task/model/saved_data/label_mapping.pkl\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    label_mapping = pickle.load(f)\n",
    "class_labels = list(label_mapping.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1344719\n",
      "Test size: 336180\n"
     ]
    }
   ],
   "source": [
    "# Display train and test sizes\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Test size: {len(X_test)}\")\n",
    "\n",
    "# Valid labels\n",
    "valid_labels = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to filter data by valid labels\n",
    "def filter_and_remap_labels(X, y, valid_labels):\n",
    "    mask = np.isin(y, valid_labels)\n",
    "    X_filtered = X[mask]\n",
    "    y_filtered = y[mask]\n",
    "    return X_filtered, y_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test set expansion function\n",
    "def expand_test_set(X_test, y_test, repeat_factor):\n",
    "    X_test_expanded = np.repeat(X_test, repeats=repeat_factor, axis=0)\n",
    "    y_test_expanded = np.repeat(y_test, repeats=repeat_factor)\n",
    "    return X_test_expanded, y_test_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the XGBoost model\n",
    "def evaluate_xgb_model(model, X_test_subset, y_test_subset, class_labels):\n",
    "    predictions = model.predict(X_test_subset)\n",
    "    accuracy = accuracy_score(y_test_subset, predictions)\n",
    "\n",
    "    unique_classes = np.unique(y_test_subset)\n",
    "    labels = list(unique_classes)\n",
    "    dynamic_labels = [class_labels[label] for label in labels]\n",
    "\n",
    "    report = classification_report(\n",
    "        y_test_subset,\n",
    "        predictions,\n",
    "        labels=labels,\n",
    "        target_names=dynamic_labels,\n",
    "        zero_division=0,\n",
    "        output_dict=True,\n",
    "    )\n",
    "    return accuracy, report, predictions, dynamic_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sizes for experiments\n",
    "sample_sizes = [25, 50, 75, 100, 250, 500, 750, 1000, 2500, 5000, 7500, 10000, 20000, 30000, 40000, 50000]\n",
    "training_sizes = [20, 40, 60, 80, 200, 400, 600, 800, 2000, 4000, 6000, 8000, 16000, 24000, 32000, 40000]\n",
    "testing_sizes = [5, 10, 15, 20, 50, 100, 150, 200, 500, 1000, 1500, 2000, 4000, 6000, 8000, 10000]\n",
    "\n",
    "# Container for results\n",
    "xgb_results = []\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "best_sample_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3], got [0 1 3 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 34\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Build and train the XGB model\u001b[39;00m\n\u001b[0;32m     28\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m XGBClassifier(\n\u001b[0;32m     29\u001b[0m     use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     30\u001b[0m     eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlogloss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     31\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m     32\u001b[0m     num_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_train_encoded)),\n\u001b[0;32m     33\u001b[0m )\n\u001b[1;32m---> 34\u001b[0m \u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     37\u001b[0m accuracy, report, predictions, dynamic_labels \u001b[38;5;241m=\u001b[39m evaluate_xgb_model(\n\u001b[0;32m     38\u001b[0m     xgb_model, X_test_expanded, y_test_encoded, class_labels\n\u001b[0;32m     39\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\syafi\\anaconda3\\envs\\mydatascienv\\lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\syafi\\anaconda3\\envs\\mydatascienv\\lib\\site-packages\\xgboost\\sklearn.py:1491\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1486\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1488\u001b[0m     classes\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1489\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (classes \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m   1490\u001b[0m ):\n\u001b[1;32m-> 1491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1492\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1493\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1494\u001b[0m     )\n\u001b[0;32m   1496\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3], got [0 1 3 4]"
     ]
    }
   ],
   "source": [
    "# Iterate through sample, training, and testing sizes\n",
    "for sample_size, train_size, test_size in zip(\n",
    "    sample_sizes, training_sizes, testing_sizes\n",
    "):\n",
    "    # Filter training data\n",
    "    X_train_subset, y_train_subset = X_train[:train_size], y_train[:train_size]\n",
    "    X_train_subset, y_train_subset = filter_and_remap_labels(\n",
    "        X_train_subset, y_train_subset, valid_labels\n",
    "    )\n",
    "\n",
    "    # Encode labels using only valid labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(valid_labels)\n",
    "    y_train_encoded = label_encoder.transform(y_train_subset)\n",
    "\n",
    "    # Expand and filter testing data\n",
    "    X_test_subset, y_test_subset = X_test[:test_size], y_test[:test_size]\n",
    "    repeat_factor = 11\n",
    "    X_test_expanded, y_test_expanded = expand_test_set(\n",
    "        X_test_subset, y_test_subset, repeat_factor\n",
    "    )\n",
    "    X_test_expanded, y_test_expanded = filter_and_remap_labels(\n",
    "        X_test_expanded, y_test_expanded, valid_labels\n",
    "    )\n",
    "    y_test_encoded = label_encoder.transform(y_test_expanded)\n",
    "\n",
    "    # Build and train the XGB model\n",
    "    xgb_model = XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric=\"mlogloss\",\n",
    "        random_state=42,\n",
    "        num_class=len(np.unique(y_train_encoded)),\n",
    "    )\n",
    "    xgb_model.fit(X_train_subset, y_train_encoded)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy, report, predictions, dynamic_labels = evaluate_xgb_model(\n",
    "        xgb_model, X_test_expanded, y_test_encoded, class_labels\n",
    "    )\n",
    "\n",
    "    # Print results for the current iteration\n",
    "    print(f\"\\nXGB Sample size {sample_size} - Accuracy: {accuracy:.4f}\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            y_test_encoded,\n",
    "            predictions,\n",
    "            labels=np.unique(y_test_encoded),\n",
    "            target_names=dynamic_labels,\n",
    "            zero_division=0,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Store results\n",
    "    report_flattened = {\n",
    "        **{\n",
    "            f\"{label}_{metric}\": value\n",
    "            for label, metrics in report.items()\n",
    "            if isinstance(metrics, dict)\n",
    "            for metric, value in metrics.items()\n",
    "        },\n",
    "        \"sample_size\": sample_size,\n",
    "        \"train_size\": train_size,\n",
    "        \"test_size\": test_size,\n",
    "        \"accuracy\": accuracy,\n",
    "    }\n",
    "    xgb_results.append(report_flattened)\n",
    "\n",
    "    # Update the best model\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = xgb_model\n",
    "        best_sample_size = sample_size\n",
    "        print(\n",
    "            f\"New best model found for sample size {sample_size} with accuracy {accuracy:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "if best_model is not None:\n",
    "    best_model_file = f\"best_xgb_model_sample_size_{best_sample_size}.pkl\"\n",
    "    with open(best_model_file, \"wb\") as model_file:\n",
    "        pickle.dump(best_model, model_file)\n",
    "    print(f\"\\nBest model saved as {best_model_file} with accuracy {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to a CSV file for further analysis\n",
    "results_df = pd.DataFrame(xgb_results)\n",
    "results_df.to_csv(\"xgb_results.csv\", index=False)\n",
    "print(\"\\nResults saved to 'xgb_results.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mydatascienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
